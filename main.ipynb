{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import math\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SQL to Neo4j\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.neo4j:neo4j-connector-apache-spark_2.12:4.0.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# JDBC URL and connection properties\n",
    "jdbc_url = \"jdbc:your_sql_database_url\"\n",
    "connection_properties = {\n",
    "    \"user\": \"your_username\",\n",
    "    \"password\": \"your_password\",\n",
    "    \"driver\": \"com.mysql.jdbc.Driver\"  # Change to your SQL database driver\n",
    "}\n",
    "\n",
    "\n",
    "# Configure Neo4j connection\n",
    "neo4j_url = \"bolt://your_neo4j_url:7687\"\n",
    "neo4j_properties = {\n",
    "    \"url\": neo4j_url,\n",
    "    \"authentication.basic.username\": \"your_neo4j_username\",\n",
    "    \"authentication.basic.password\": \"your_neo4j_password\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the min and max values of the partition column\n",
    "def get_min_max(table, column):\n",
    "    query = f\"(SELECT MIN({column}) as min_val, MAX({column}) as max_val FROM {table}) as min_max_table\"\n",
    "    df = spark.read.jdbc(url=jdbc_url, table=query, properties=connection_properties)\n",
    "    row = df.collect()[0]\n",
    "    return row[\"min_val\"], row[\"max_val\"]\n",
    "\n",
    "# Function to calculate the number of partitions\n",
    "def calculate_num_partitions(size, partition_size=100000):\n",
    "    return math.ceil(size / partition_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = {\n",
    "    \"actors\": {\"column\": \"id\", \"df\": None},\n",
    "    \"genre\": {\"column\": \"id\", \"df\": None},\n",
    "    \"movie\": {\"column\": \"id\", \"df\": None},\n",
    "    \"acted_in\": {\"column\": \"id\", \"df\": None},\n",
    "    \"awards\": {\"column\": \"id\", \"df\": None}\n",
    "}\n",
    "\n",
    "for table, info in tables.items():\n",
    "    min_val, max_val = get_min_max(table, info[\"column\"])\n",
    "    size = max_val - min_val + 1\n",
    "    num_partitions = calculate_num_partitions(size)\n",
    "    \n",
    "    info[\"df\"] = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=table,\n",
    "        properties=connection_properties,\n",
    "        column=info[\"column\"],\n",
    "        lowerBound=min_val,\n",
    "        upperBound=max_val,\n",
    "        numPartitions=num_partitions\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract DataFrames from the dictionary\n",
    "actors_df = tables[\"actors\"][\"df\"]\n",
    "genre_df = tables[\"genre\"][\"df\"]\n",
    "movie_df = tables[\"movie\"][\"df\"]\n",
    "acted_in_df = tables[\"acted_in\"][\"df\"]\n",
    "awards_df = tables[\"awards\"][\"df\"]\n",
    "\n",
    "# Transform data\n",
    "actors_df = actors_df.withColumnRenamed(\"id\", \"actorId\")\n",
    "genre_df = genre_df.withColumnRenamed(\"id\", \"genreId\")\n",
    "movie_df = movie_df.withColumnRenamed(\"id\", \"movieId\").withColumnRenamed(\"genre_id\", \"genreId\")\n",
    "acted_in_df = acted_in_df.withColumnRenamed(\"actor_id\", \"actorId\").withColumnRenamed(\"movie_id\", \"movieId\")\n",
    "awards_df = awards_df.withColumnRenamed(\"actor_id\", \"actorId\").withColumnRenamed(\"movie_id\", \"movieId\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to write data to Neo4j\n",
    "def write_to_neo4j(df, labels, relationship=None):\n",
    "    df.write \\\n",
    "        .format(\"org.neo4j.spark.DataSource\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"url\", neo4j_url) \\\n",
    "        .option(\"labels\", labels) \\\n",
    "        .option(\"relationship\", relationship) \\\n",
    "        .option(\"relationship.save.strategy\", \"keys\") \\\n",
    "        .option(\"relationship.source.labels\", \":Actor\" if relationship else None) \\\n",
    "        .option(\"relationship.source.save.mode\", \"overwrite\" if relationship else None) \\\n",
    "        .option(\"relationship.target.labels\", \":Movie\" if relationship else None) \\\n",
    "        .option(\"relationship.target.save.mode\", \"overwrite\" if relationship else None) \\\n",
    "        .option(\"relationship.source.node.keys\", \"actorId:actorId\" if relationship else None) \\\n",
    "        .option(\"relationship.target.node.keys\", \"movieId:movieId\" if relationship else None) \\\n",
    "        .save()\n",
    "\n",
    "# Write data to Neo4j with partitioning\n",
    "actors_df.repartition(num_partitions).foreachPartition(lambda df: write_to_neo4j(df, \":Actor\"))\n",
    "genre_df.repartition(num_partitions).foreachPartition(lambda df: write_to_neo4j(df, \":Genre\"))\n",
    "movie_df.repartition(num_partitions).foreachPartition(lambda df: write_to_neo4j(df, \":Movie\"))\n",
    "acted_in_df.repartition(num_partitions).foreachPartition(lambda df: write_to_neo4j(df, \":Actor\", \"ACTED_IN\"))\n",
    "awards_df.repartition(num_partitions).foreachPartition(lambda df: write_to_neo4j(df, \":Actor\", \"AWARDED\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
